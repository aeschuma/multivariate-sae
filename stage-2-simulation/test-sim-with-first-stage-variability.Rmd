---
title: "Simulation with first stage variability"
author: "Austin Schumacher"
date: "`r Sys.Date()`"
output: 
    pdf_document:
        extra_dependencies: ["bm", "bbm", "tensor", "pdfpages"]
        includes:
            in_header: "~/Desktop/work/presentation_publication_resources/mycommands.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(SUMMER)
library(tidyverse)
library(spdep)
library(geosphere)
library(haven)
library(knitr)
library(kableExtra)
library(magrittr)
library(rstan)
library(cmdstanr)
library(svyVGAM)
library(mvtnorm)
library(rgdal)
library(bayesplot)
library(INLA)
library(viridis)
library(classInt)
library(gridExtra)
library(ggpubr)
library(loo)

dropbox_dir <- "~/Dropbox/dissertation_2/survey-csmf/results/stage-2-simulation"
```

# Overview

We will perform a simulation study to explore the recovery of parameters. This simulation will include the variability from the first-stage model in order to validate the full model (rather than taking the first stage results as fixed and just validating the second stage model). Instead of simulating individual-level data and calculating first-stage direct estimates and fitting a second stage smoothing model to those, we will set known means and variances for each region and then draw area-level sample means from their sampling distribution, and additionally draw area-level sample variances their sampling distribution. The known mean parameters will be parameterized by smoothing models that we will fit in the second stage. Pseudo-direct estimates will be these area-level sample means with associated variance equal to the these sample variances. For each region, we will assess the bias of the sample means and coverage of uncertainty intervals created with the sample variances. We will then fit our second-stage smoothing models on these sample means and sample variances and assess the bias and coverage of the estimated latent means in each region.

# 1-dimensional data

We will start with the simplest case of one-dimensional data.

## IID latent mean model

Before we try simulating data with both IID and ICAR components, we will first look at the IID-only case.

### Data generating mechanism

Let $r = 1, \dots, R$ index region. The region-level means will be generated as

\eq{
    \mu_{r} &= \beta_1 + v_{r} \\
    v_{r} &\overset{iid}{\sim} N(0,\sigma^2_v).
}

with $\beta_1$ set to be the estimated intercept and $\sigma^2_v$ set to be the estimated variance of the random effects from a model fit to the 2014 KDHS HAZ data. The random effects will only be simulated once and the same ones will be used across all simulations.

Once we have these $\mu_{r}$ values, we will simulate the area-level sample means as

\eq{
    \hat{y}_r | \mu_r, V_r &\sim N(\mu_r, V_r)
}

where the $V_r$ are set to be equal to the asymptotic design-based variance estimate, $\hat{V}^{des}_r$, of the area-level mean HAZ from the 2014 KDHS data.

Now, for an SRS, the sampling distribution of the area-level sample variances is

\eq{
    \hat{V}^{srs}_r | V_r, n_r \sim \Gamma\left(\frac{n_r - 1}{2}, \frac{n_r - 1}{2 V_r}\right) 
}

where $n_r$ is the sample size of an SRS in area $r$. The 2014 KDHS uses a stratified cluster design rather than an SRS, which has a different sampling variance than an SRS. The ratio of the variance for a statistic calculated using a specific survey design to the variance of that statistic calculated using an SRS of the same sample size is called the design effect,

\eq{
    d^2 &= \frac{\hat{V}}{\hat{V}^{srs}}.
}

In the a typical DHS, the average design effect across all indicators is $d^2 = 1.5^2 = 2.25$ (taken from this answer on the DHS user forum: https://userforum.dhsprogram.com/index.php?t=msg&goto=3448&S=Google). Thus, for our simulation we will let $n_r$ be the number of sampled children with HAZ scores in each region, and we will calculate the effective sample size that would be needed from an SRS in order to have the same sampling variance as the DHS sampling design. 

Since the variance of a $\Gamma(\alpha, \beta)$ distribution is $\alpha/\beta^2$, we have

\eq{
    \Var(\hat{V}^{srs}_r) &= \frac{\left(\frac{n_r-1}{2}\right)}{\left(\frac{n_r-1}{2 V_r}\right)^2} \\
    &= \frac{2 (V_r)^2}{(n_r - 1)}.
}

Now, we want $\hat{V}_r = 2.25 \times \hat{V}^{srs}_r = 2.25 \times \frac{2 (V_r)^2}{(n_r - 1)} = \frac{2 (V_r)^2}{\left((n_r - 1)/2.25\right)} \approx \frac{2 (V_r)^2}{\left((n_r/2.25 - 1)\right)}$, which yields an effective sample size of $n_r^* = n_r/2.25$ (as long as $n_r$ is sufficiently large). 

Thus, we will simulate the area-level sample variances as

\eq{
    \hat{V}_r | V_r, n_r^* \sim \Gamma\left(\frac{(n_r^* - 1)}{2}, \frac{n_r^* - 1}{2 V_r}\right).
}

In each simulation, we will have the same values of $\mu_r$ and $V_r$, and we use these to simulate $\hat{y}_r$ and $\hat{V}_r$. We will treat the $\hat{y}_r$ as pseudo-direct estimates and calculate asymptotic 95\% confidence intervals as $\hat{y}_r \pm t_{n_r^*-1, 0.975}\sqrt{\hat{V}_r}$. We will also fit an IID smoothing model to the pseudo-direct estimates to estimate the latent means in each region with corresponding 95\% credible intervals. This smoothing model will be correctly specified to match the data generating mechanism and be fit via a Bayesian model with relatively uninformative priors.

### Results

```{r, fig.height=9, cache = TRUE}
# functions ####
fitINLA <- function(formula, data, lincombs) {
    inla(formula, data = data, 
         family = "gaussian",
         lincomb = lincombs,
         control.family = list(hyper = list(prec = list(initial = 10, fixed=T))), 
         control.predictor = list(compute=T),
         control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
         control.inla = list(lincomb.derived.correlation.matrix=T),
         control.fixed = list(prec = list(default = 0.001), correlation.matrix=T))
}

# load raw data
load(paste0(dropbox_dir, "/../../data/ken_dhs2014/data/haz-waz-kenDHS2014.rda"))
n_regions <- length(unique(dat$admin1))

# extract values from data
n_r <- table(dat$admin1)
d <- 1.5
n_r_star <- n_r/(d^2)
# load direct estimates
direct_estimates <- read_rds(paste0(dropbox_dir,"/../ken2014-hazwaz/ken2014-hazwaz-stage-1.rds"))
V_r <- direct_estimates[["V.array"]][,1,1]

# load 2014 KDHS HAZ/WAZ modeling results
inla_results <- readRDS(paste0(dropbox_dir, "/../ken2014-hazwaz/ken2014-hazwaz-stage-2-inla-all.rds"))

# extract parameters from model for sim
beta_1 <- inla_results[["Univariate IID"]]$summary.fixed$`0.5quant`[1]
sigma <- (inla_results[["Univariate IID"]]$summary.hyperpar$`0.5quant`[1])^{-1/2}

# simulate a single set of random effects
set.seed(8008135)
v_r <- rnorm(n_regions, 0, sigma)

# calculate latent means
mu_r <- beta_1 + v_r

# what measures do we want to calculate
measures <- c("mean bias",
              "mean absolute bias",
              "mean relative bias",
              "80% coverage",
              "95% coverage",
              "80% width",
              "95% width")

# start simulations
set.seed(80085)
n_sim <- 500
sim_results <- list(direct_estimates = NA,
                    univariate_iid = NA)
for (i in 1:length(sim_results)) {
    sim_results[[i]] <- array(NA, dim = c(length(measures), n_sim, n_regions))
    # sim_results[[i]] <- tibble(bias = rep(NA, n_sim),
    #                           abs_bias = rep(NA, n_sim),
    #                           rel_abs_bias = rep(NA, n_sim),
    #                           coverage_80 = rep(NA, n_sim),
    #                           width_80 = rep(NA, n_sim),
    #                           coverage_95 = rep(NA, n_sim),
    #                           width_95 = rep(NA, n_sim))
}

for(s in 1:n_sim) {
    # simulate direct estimates
    y_hat_r <- rnorm(n_regions, mu_r, sqrt(V_r))
    
    # simulate V_des
    V_hat_r <- rgamma(n_regions, (n_r_star - 1)/2, rate = (n_r_star - 1)/(2*V_r))
    
    # calculate bias results for direct estimates
    sim_results$direct_estimates[1, s, ] <- y_hat_r - mu_r
    sim_results$direct_estimates[2, s, ] <- abs(y_hat_r - mu_r)
    sim_results$direct_estimates[3, s, ] <- abs(y_hat_r - mu_r)/abs(mu_r)
    
    # calculate CIs
    ci_80_direct <- t(rbind(y_hat_r, y_hat_r) + t(t(c(-1, 1))) %*% t(qt(0.9, n_r_star-1)*sqrt(V_hat_r)))
    ci_95_direct <- t(rbind(y_hat_r, y_hat_r) + t(t(c(-1, 1))) %*% t(qt(0.975, n_r_star-1)*sqrt(V_hat_r)))
    
    # calculate coverage and width results
    sim_results$direct_estimates[4, s, ] <- (mu_r >= ci_80_direct[,1]) & (mu_r <= ci_80_direct[,2])
    sim_results$direct_estimates[5, s, ] <- (mu_r >= ci_95_direct[,1]) & (mu_r <= ci_95_direct[,2])
    sim_results$direct_estimates[6, s, ] <- (ci_80_direct[,2] - ci_80_direct[,1])
    sim_results$direct_estimates[7, s, ] <- (ci_95_direct[,2] - ci_95_direct[,1])
    
    # prep for inla model
    data <- list(y = y_hat_r,
                 admin1 = 1:n_regions)
    formula <- as.formula("y ~ 1 + f(admin1, model = 'iid')")
    scaling <- 1/V_hat_r

    # fit smoothing models
    mod1 <- inla(formula,
                 family = "gaussian",
                 data = data,
                 control.family = list(hyper = list(prec = list(initial = log(1), fixed=T))),
                 control.predictor = list(compute=T),
                 control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
                 control.inla = list(lincomb.derived.correlation.matrix=T),
                 scale = scaling,
                 quantiles = c(0.025, 0.1, 0.5, 0.9, 0.975))
    fitted_res <- mod1$summary.fitted.values
    latent_means <- fitted_res$`0.5quant`

    # calculate bias results for direct estimates
    sim_results$univariate_iid[1, s, ] <- latent_means - mu_r
    sim_results$univariate_iid[2, s, ] <- abs(latent_means - mu_r)
    sim_results$univariate_iid[3, s, ] <- abs(latent_means - mu_r)/abs(mu_r)

    # calculate coverage and width results
    sim_results$univariate_iid[4, s, ] <- (mu_r >= fitted_res$`0.1quant`) & (mu_r <= fitted_res$`0.9quant`)
    sim_results$univariate_iid[5, s, ] <- (mu_r >= fitted_res$`0.025quant`) & (mu_r <= fitted_res$`0.975quant`)
    sim_results$univariate_iid[6, s, ] <- (fitted_res$`0.9quant` - fitted_res$`0.1quant`)
    sim_results$univariate_iid[7, s, ] <- (fitted_res$`0.975quant` - fitted_res$`0.025quant`)
}

# compile results
sim_results_region <- lapply(sim_results, function(x) {t(apply(x, c(1,3), mean))})
sim_results_compiled <- lapply(sim_results_region, function(x) {apply(x, 2, mean)})

# plot region-level results
par(mfrow = c(ceiling(length(measures)/2), 2))
for (i in 1:length(measures)) {
    plot(sim_results_region$univariate_iid[, i] ~ sim_results_region$direct_estimates[, i],
         pch = 19, col = rgb(0, 0, 0, alpha = 0.5), xlab = "direct estimates", ylab = "IID models",
         main = measures[i])
    abline(0, 1, col = "darkgreen")
}

```

```{r}
sim_results_table <- tibble(measure = measures,
                            direct_estimates = sim_results_compiled$direct_estimates,
                            univariate_iid = sim_results_compiled$univariate_iid)
kable(sim_results_table, format = 'markdown', col.names = c("measure", "Direct", "IID model"), digits = 4,
      caption = paste0("Simulation results for ", n_sim, " simulations, univariate DGM"))
```

# 2-dimensional data

## IID latent mean model

Before we try simulating data with both IID and ICAR components, we will first look at the IID-only case.

### Data generating mechanism

Let $r = 1, \dots, R$ index region and $c = 1, 2$ index outcomes. The region-level means will be generated as

\eq{
    \mu_{rc} &= \beta_c + v_{rc} \\
    v_{rc} &\overset{iid}{\sim} N(0,\sigma^2_c).
}

with $\beta_c$ set to be the estimated intercept and $\sigma^2_c$ set to be the estimated variances of the random effects from a model fit to the 2014 KDHS HAZ and WAZ data. The random effects will only be simulated once and the same ones will be used across all simulations.

Once we have these $\mu_{rc}$ values, we will simulate the area-level sample means as

\eq{
    \hat{\bm{y}}_{r} | \bm\mu_{r}, \bm{V}_r &\sim N_2(\bm\mu_r, \bm{V}_r)
}

where the $\bm{V}_r$ are set to be equal to the estimated asymptotic design-based covariance matrix, $\hat{V}^{des}_r$, of the area-level mean HAZ and WAZ from the 2014 KDHS data.

Now, for an SRS, the sampling distribution of the area-level sample covariance matrices is

\eq{
    (n_r - 1) \hat{\bm{V}}^{srs}_r | V_r, n_r \sim \text{Wishart}\left( \bm{V}_r, (n_r-1) \right) 
}

where $n_r$ is the sample size of an SRS in area $r$. We will again adjust for the survey design effect, giving us

\eq{
    \hat{\bm{V}}^{srs}_r | V_r, n_r \sim \frac{1}{(n^*_r - 1)}\text{Wishart}\left( \bm{V}_r, (n^*_r - 1) \right) 
}

In each simulation, we will have the same values of $\mu_r$ and $V_r$, and we use these to simulate $\hat{y}_r$ and $\hat{V}_r$. We will treat the $\hat{y}_r$ as pseudo-direct estimates and calculate asymptotic 95\% confidence intervals as $\hat{y}_r \pm t_{n_r^*-1, 0.975}\sqrt{\hat{V}_r}$. We will also fit an IID smoothing model to the pseudo-direct estimates to estimate the latent means in each region with corresponding 95\% credible intervals. This smoothing model will be correctly specified to match the data generating mechanism and be fit via a Bayesian model with relatively uninformative priors.

```{r}
# functions ####
fitINLA <- function(formula, data, lincombs) {
    inla(formula, data = data, 
         family = "gaussian",
         lincomb = lincombs,
         control.family = list(hyper = list(prec = list(initial = 10, fixed=T))), 
         control.predictor = list(compute=T),
         control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
         control.inla = list(lincomb.derived.correlation.matrix=T),
         control.fixed = list(prec = list(default = 0.001), correlation.matrix=T))
}

# load raw data
load(paste0(dropbox_dir, "/../../data/ken_dhs2014/data/haz-waz-kenDHS2014.rda"))
n_regions <- length(unique(dat$admin1))

# extract values from data
n_r <- table(dat$admin1)
d <- 1.5
n_r_star <- n_r/(d^2)
# load direct estimates
direct_estimates <- read_rds(paste0(dropbox_dir,"/../ken2014-hazwaz/ken2014-hazwaz-stage-1.rds"))
V_r <- direct_estimates[["V.array"]]

# load 2014 KDHS HAZ/WAZ modeling results
inla_results <- readRDS(paste0(dropbox_dir, "/../ken2014-hazwaz/ken2014-hazwaz-stage-2-inla-all.rds"))

# extract parameters from model for sim
beta <- inla_results[["Bivariate nonshared IID"]]$summary.fixed$`0.5quant`
sigma <- (inla_results[["Bivariate nonshared IID"]]$summary.hyperpar$`0.5quant`)^{-1/2}

# simulate a single set of random effects
set.seed(8008135)
v_r_1 <- rnorm(n_regions, 0, sigma[1])
v_r_2 <- rnorm(n_regions, 0, sigma[2])

# calculate latent means
mu_r_1 <- beta[1] + v_r_1
mu_r_2 <- beta[2] + v_r_2
mu_r <- cbind(mu_r_1, mu_r_2)

# what measures do we want to calculate
measures <- c("mean bias",
              "mean absolute bias",
              "mean relative bias",
              "80% coverage",
              "95% coverage",
              "80% width",
              "95% width")

# start simulations
set.seed(80085)
n_sim <- 100
sim_results <- list(direct_estimates = NA,
                    univariate_iid = NA)
for (i in 1:length(sim_results)) {
    sim_results[[i]] <- array(NA, dim = c(length(measures), n_sim, n_regions*2))
    # sim_results[[i]] <- tibble(bias = rep(NA, n_sim),
    #                           abs_bias = rep(NA, n_sim),
    #                           rel_abs_bias = rep(NA, n_sim),
    #                           coverage_80 = rep(NA, n_sim),
    #                           width_80 = rep(NA, n_sim),
    #                           coverage_95 = rep(NA, n_sim),
    #                           width_95 = rep(NA, n_sim))
}

for(s in 1:n_sim) {
    
    # simulate direct estimates
    y_hat_r <- matrix(NA, nrow = n_regions, ncol = 2)
    V_hat_r <- V_r
    for (i in 1:n_regions) {
        y_hat_r[i,] <- rmvnorm(1, mu_r[i,], V_r[i, , ])
    
        # simulate V_des
        V_hat_r[i,,] <- rWishart(1, (n_r_star[i] - 1), V_r[i,,])/(n_r_star[i] - 1)
    }
    
    # calculate bias results for direct estimates
    sim_results$direct_estimates[1, s, ] <- as.vector(y_hat_r - mu_r)
    sim_results$direct_estimates[2, s, ] <- abs(as.vector(y_hat_r) - as.vector(mu_r))
    sim_results$direct_estimates[3, s, ] <- abs(as.vector(y_hat_r) - as.vector(mu_r))/abs(as.vector(mu_r))
    
    # calculate CIs
    ci_80_direct <- t(rbind(as.vector(y_hat_r), as.vector(y_hat_r)) + t(t(c(-1, 1))) %*% t(qnorm(0.9)*sqrt(c(V_hat_r[, 1, 1], V_hat_r[, 2, 2]))))
    ci_95_direct <- t(rbind(as.vector(y_hat_r), as.vector(y_hat_r)) + t(t(c(-1, 1))) %*% t(qnorm(0.975)*sqrt(c(V_hat_r[, 1, 1], V_hat_r[, 2, 2]))))
    
    # calculate coverage and width results
    sim_results$direct_estimates[4, s, ] <- (as.vector(mu_r) >= ci_80_direct[,1]) & (as.vector(mu_r) <= ci_80_direct[,2])
    sim_results$direct_estimates[5, s, ] <- (as.vector(mu_r) >= ci_95_direct[,1]) & (as.vector(mu_r) <= ci_95_direct[,2])
    sim_results$direct_estimates[6, s, ] <- (ci_80_direct[,2] - ci_80_direct[,1])
    sim_results$direct_estimates[7, s, ] <- (ci_95_direct[,2] - ci_95_direct[,1])
    
    # # prep for inla model
    # data <- list(y = y_hat_r,
    #              admin1 = 1:n_regions)
    # formula <- as.formula("y ~ 1 + f(admin1, model = 'iid')")
    # scaling <- 1/V_hat_r
    # 
    # # fit smoothing models
    # mod1 <- inla(formula,
    #              family = "gaussian",
    #              data = data,
    #              control.family = list(hyper = list(prec = list(initial = log(1), fixed=T))),
    #              control.predictor = list(compute=T),
    #              control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
    #              control.inla = list(lincomb.derived.correlation.matrix=T),
    #              scale = scaling,
    #              quantiles = c(0.025, 0.1, 0.5, 0.9, 0.975))
    # fitted_res <- mod1$summary.fitted.values
    # latent_means <- fitted_res$`0.5quant`
    # 
    # # calculate bias results for direct estimates
    # sim_results$univariate_iid[1, s, ] <- latent_means - mu_r
    # sim_results$univariate_iid[2, s, ] <- abs(latent_means - mu_r)
    # sim_results$univariate_iid[3, s, ] <- abs(latent_means - mu_r)/abs(mu_r)
    # 
    # # calculate coverage and width results
    # sim_results$univariate_iid[4, s, ] <- (mu_r >= fitted_res$`0.1quant`) & (mu_r <= fitted_res$`0.9quant`)
    # sim_results$univariate_iid[5, s, ] <- (mu_r >= fitted_res$`0.025quant`) & (mu_r <= fitted_res$`0.975quant`)
    # sim_results$univariate_iid[6, s, ] <- (fitted_res$`0.9quant` - fitted_res$`0.1quant`)
    # sim_results$univariate_iid[7, s, ] <- (fitted_res$`0.975quant` - fitted_res$`0.025quant`)
}

# compile results
sim_results_region <- lapply(sim_results, function(x) {t(apply(x, c(1,3), mean))})
sim_results_compiled <- lapply(sim_results_region, function(x) {apply(x, 2, mean)})

# plot region-level results
par(mfrow = c(ceiling(length(measures)/2), 2))
for (i in 1:length(measures)) {
    plot(sim_results_region$univariate_iid[, i] ~ sim_results_region$direct_estimates[, i],
         pch = 19, col = rgb(0, 0, 0, alpha = 0.5), xlab = "direct estimates", ylab = "IID models",
         main = measures[i])
    abline(0, 1, col = "darkgreen")
}
```

## IID + ICAR latent mean model

Now we will fit IID + ICAR models

### Results

```{r}
# functions ####
fitINLA <- function(formula, data, lincombs) {
    inla(formula, data = data, 
         family = "gaussian",
         lincomb = lincombs,
         control.family = list(hyper = list(prec = list(initial = 10, fixed=T))), 
         control.predictor = list(compute=T),
         control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
         control.inla = list(lincomb.derived.correlation.matrix=T),
         control.fixed = list(prec = list(default = 0.001), correlation.matrix=T))
}

# load raw data
load(paste0(dropbox_dir, "/../../data/ken_dhs2014/data/haz-waz-kenDHS2014.rda"))
n_regions <- length(unique(dat$admin1))

# extract values from data
n_r <- table(dat$admin1)
d <- 1.5
n_r_star <- n_r/(d^2)
# load direct estimates
direct_estimates <- read_rds(paste0(dropbox_dir,"/../ken2014-hazwaz/ken2014-hazwaz-stage-1.rds"))
V_r <- direct_estimates[["V.array"]]

# load 2014 KDHS HAZ/WAZ modeling results
inla_results <- readRDS(paste0(dropbox_dir, "/../ken2014-hazwaz/ken2014-hazwaz-stage-2-inla-all.rds"))

# extract parameters from model for sim
beta <- inla_results[["Bivariate nonshared BYM"]]$summary.fixed$`0.5quant`
sigma <- (inla_results[["Bivariate nonshared BYM"]]$summary.hyperpar[c("Precision for admin1.haz", "Precision for admin1.waz"), "0.5quant"])^{-1/2}
rho <- (inla_results[["Bivariate nonshared BYM"]]$summary.hyperpar[c("Phi for admin1.haz", "Phi for admin1.waz"), "0.5quant"])

# simulate a single set of random effects
set.seed(8008135)
v_1 <- rnorm(n_regions, 0, 1)
v_2 <- rnorm(n_regions, 0, 1)

Q <- admin1.mat * -1
diag(Q) <- 0
diag(Q) <- -1 * apply(Q, 2, sum)

u_1 <- sim.Q(Q)
u_2 <- sim.Q(Q) 

## Convolved REs
convolved_re_1 <- (sqrt(1 - rho[1]) * v_1) + (sqrt(rho[1] / scaling_factor) * u_1)
convolved_re_2 <- (sqrt(1 - rho[2]) * v_2) + (sqrt(rho[2] / scaling_factor) * u_2)

# calculate latent means
mu_r_1 <- beta[1] + convolved_re_1 * sigma[1]
mu_r_2 <- beta[2] + convolved_re_2 * sigma[2]
mu_r <- cbind(mu_r_1, mu_r_2)

# what measures do we want to calculate
measures <- c("mean bias",
              "mean absolute bias",
              "mean relative bias",
              "80% coverage",
              "95% coverage",
              "80% width",
              "95% width")

# start simulations
set.seed(80085)
n_sim <- 100
sim_results <- list(direct_estimates = NA,
                    univariate_iid = NA)
for (i in 1:length(sim_results)) {
    sim_results[[i]] <- array(NA, dim = c(length(measures), n_sim, n_regions*2))
    # sim_results[[i]] <- tibble(bias = rep(NA, n_sim),
    #                           abs_bias = rep(NA, n_sim),
    #                           rel_abs_bias = rep(NA, n_sim),
    #                           coverage_80 = rep(NA, n_sim),
    #                           width_80 = rep(NA, n_sim),
    #                           coverage_95 = rep(NA, n_sim),
    #                           width_95 = rep(NA, n_sim))
}

for(s in 1:n_sim) {
    
    # simulate direct estimates
    y_hat_r <- matrix(NA, nrow = n_regions, ncol = 2)
    V_hat_r <- V_r
    for (i in 1:n_regions) {
        y_hat_r[i,] <- rmvnorm(1, mu_r[i,], V_r[i, , ])
    
        # simulate V_des
        V_hat_r[i,,] <- rWishart(1, (n_r_star[i] - 1), V_r[i,,])/(n_r_star[i] - 1)
    }
    
    # calculate bias results for direct estimates
    sim_results$direct_estimates[1, s, ] <- as.vector(y_hat_r - mu_r)
    sim_results$direct_estimates[2, s, ] <- abs(as.vector(y_hat_r) - as.vector(mu_r))
    sim_results$direct_estimates[3, s, ] <- abs(as.vector(y_hat_r) - as.vector(mu_r))/abs(as.vector(mu_r))
    
    # calculate CIs
    ci_80_direct <- t(rbind(as.vector(y_hat_r), as.vector(y_hat_r)) + t(t(c(-1, 1))) %*% t(qnorm(0.9)*sqrt(c(V_hat_r[, 1, 1], V_hat_r[, 2, 2]))))
    ci_95_direct <- t(rbind(as.vector(y_hat_r), as.vector(y_hat_r)) + t(t(c(-1, 1))) %*% t(qnorm(0.975)*sqrt(c(V_hat_r[, 1, 1], V_hat_r[, 2, 2]))))
    
    # calculate coverage and width results
    sim_results$direct_estimates[4, s, ] <- (as.vector(mu_r) >= ci_80_direct[,1]) & (as.vector(mu_r) <= ci_80_direct[,2])
    sim_results$direct_estimates[5, s, ] <- (as.vector(mu_r) >= ci_95_direct[,1]) & (as.vector(mu_r) <= ci_95_direct[,2])
    sim_results$direct_estimates[6, s, ] <- (ci_80_direct[,2] - ci_80_direct[,1])
    sim_results$direct_estimates[7, s, ] <- (ci_95_direct[,2] - ci_95_direct[,1])
    
    # # prep for inla model
    # data <- list(y = y_hat_r,
    #              admin1 = 1:n_regions)
    # formula <- as.formula("y ~ 1 + f(admin1, model = 'iid')")
    # scaling <- 1/V_hat_r
    # 
    # # fit smoothing models
    # mod1 <- inla(formula,
    #              family = "gaussian",
    #              data = data,
    #              control.family = list(hyper = list(prec = list(initial = log(1), fixed=T))),
    #              control.predictor = list(compute=T),
    #              control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
    #              control.inla = list(lincomb.derived.correlation.matrix=T),
    #              scale = scaling,
    #              quantiles = c(0.025, 0.1, 0.5, 0.9, 0.975))
    # fitted_res <- mod1$summary.fitted.values
    # latent_means <- fitted_res$`0.5quant`
    # 
    # # calculate bias results for direct estimates
    # sim_results$univariate_iid[1, s, ] <- latent_means - mu_r
    # sim_results$univariate_iid[2, s, ] <- abs(latent_means - mu_r)
    # sim_results$univariate_iid[3, s, ] <- abs(latent_means - mu_r)/abs(mu_r)
    # 
    # # calculate coverage and width results
    # sim_results$univariate_iid[4, s, ] <- (mu_r >= fitted_res$`0.1quant`) & (mu_r <= fitted_res$`0.9quant`)
    # sim_results$univariate_iid[5, s, ] <- (mu_r >= fitted_res$`0.025quant`) & (mu_r <= fitted_res$`0.975quant`)
    # sim_results$univariate_iid[6, s, ] <- (fitted_res$`0.9quant` - fitted_res$`0.1quant`)
    # sim_results$univariate_iid[7, s, ] <- (fitted_res$`0.975quant` - fitted_res$`0.025quant`)
}

# compile results
sim_results_region <- lapply(sim_results, function(x) {t(apply(x, c(1,3), mean))})
sim_results_compiled <- lapply(sim_results_region, function(x) {apply(x, 2, mean)})

# plot region-level results
par(mfrow = c(ceiling(length(measures)/2), 2))
for (i in 1:length(measures)) {
    plot(sim_results_region$univariate_iid[, i] ~ sim_results_region$direct_estimates[, i],
         pch = 19, col = rgb(0, 0, 0, alpha = 0.5), xlab = "direct estimates", ylab = "IID models",
         main = measures[i])
    abline(0, 1, col = "darkgreen")
}
```

## Shared IID + ICAR latent mean model

Now we will fit shared IID + ICAR models

### Results

```{r}
# functions ####
fitINLA <- function(formula, data, lincombs) {
    inla(formula, data = data, 
         family = "gaussian",
         lincomb = lincombs,
         control.family = list(hyper = list(prec = list(initial = 10, fixed=T))), 
         control.predictor = list(compute=T),
         control.compute = list(config=T, waic = TRUE, dic = TRUE, cpo = TRUE),
         control.inla = list(lincomb.derived.correlation.matrix=T),
         control.fixed = list(prec = list(default = 0.001), correlation.matrix=T),
         quantiles=c(0.025, 0.1, 0.5, 0.9, 0.975))
}

# load raw data
load(paste0(dropbox_dir, "/../../data/ken_dhs2014/data/haz-waz-kenDHS2014.rda"))
n_regions <- length(unique(dat$admin1))

# extract values from data
n_r <- table(dat$admin1)
d <- 1.5
n_r_star <- n_r/(d^2)
# load direct estimates
direct_estimates <- read_rds(paste0(dropbox_dir,"/../ken2014-hazwaz/ken2014-hazwaz-stage-1.rds"))
V_r <- direct_estimates[["V.array"]]

# load 2014 KDHS HAZ/WAZ modeling results
inla_results <- readRDS(paste0(dropbox_dir, "/../ken2014-hazwaz/ken2014-hazwaz-stage-2-inla-all.rds"))

# extract parameters from model for sim
beta <- inla_results[["Bivariate shared BYM"]]$summary.fixed$`0.5quant`
sigma <- (inla_results[["Bivariate shared BYM"]]$summary.hyperpar[c("Precision for admin1.haz", "Precision for admin1.waz"), "0.5quant"])^{-1/2}
rho <- (inla_results[["Bivariate shared BYM"]]$summary.hyperpar[c("Phi for admin1.haz", "Phi for admin1.waz"), "0.5quant"])
lambda <- inla_results[["Bivariate shared BYM"]]$summary.hyperpar[c("Beta for admin1.haz.2"), "0.5quant"]

# simulate a single set of random effects
set.seed(80085)
v_1 <- rnorm(n_regions, 0, 1)
v_2 <- rnorm(n_regions, 0, 1)

Q <- admin1.mat * -1
diag(Q) <- 0
diag(Q) <- -1 * apply(Q, 2, sum)

sim.Q <- function(Q){
    eigenQ <- eigen(Q)
    rankQ <- qr(Q)$rank
    sim <- as.vector(eigenQ$vectors[,1:rankQ] %*% 
                         matrix(
                             stats::rnorm(rep(1, rankQ), rep(0, rankQ), 1/sqrt(eigenQ$values[1:rankQ])),
                             ncol = 1))
    sim
}

u_1 <- sim.Q(Q)
u_2 <- sim.Q(Q) 

## Convolved REs
convolved_re_1 <- (sqrt(1 - rho[1]) * v_1) + (sqrt(rho[1] / scaling_factor) * u_1)
convolved_re_2 <- (sqrt(1 - rho[2]) * v_2) + (sqrt(rho[2] / scaling_factor) * u_2)

# calculate latent means
mu_r_1 <- beta[1] + (convolved_re_1 * sigma[1]) + (lambda * convolved_re_2 * sigma[2])
mu_r_2 <- beta[2] + (convolved_re_2 * sigma[2])
mu_r <- cbind(mu_r_1, mu_r_2)

# what measures do we want to calculate
measures <- c("mean bias",
              "mean absolute bias",
              "mean relative bias",
              "80% coverage",
              "95% coverage",
              "80% width",
              "95% width")

# start simulations
set.seed(8008135)
n_sim <- 50

sim_results <- list(direct_estimates = NA,
                    bivariate_shared_bym = NA)
for (i in 1:length(sim_results)) {
    sim_results[[i]] <- array(NA, dim = c(length(measures), n_sim, n_regions*2))
    # sim_results[[i]] <- tibble(bias = rep(NA, n_sim),
    #                           abs_bias = rep(NA, n_sim),
    #                           rel_abs_bias = rep(NA, n_sim),
    #                           coverage_80 = rep(NA, n_sim),
    #                           width_80 = rep(NA, n_sim),
    #                           coverage_95 = rep(NA, n_sim),
    #                           width_95 = rep(NA, n_sim))
}

# define model components ####

## priors ####
iid_prior <- list(prec = list(prior = "pc.prec",
                              param = c(1, 0.01)))
bym2_prior <- list(phi=list(prior="logitbeta", param=c(1, 1), initial=0.5), 
                   prec=list(prior="pc.prec", param=c(1, 0.01), initial=5))
lambda_prior <- list(beta = list(prior = 'logtnormal', param = c(0, 1)))

## Diagonal V formula additions ####

### we now have to add one 'iid2d' model for each observation pair,
### since their cov.matrix is different. we have to do this
### automatically... here I add numbers directly for simplicity

### bivariate
add.bivariate <- ""
for(j in 1:Nt) {
    corr <-  results$corr.bi[j]
    init.prec.haz <-  log(results$seHAZ.bi[j]^(-2))
    init.prec.waz <-  log(results$seWAZ.bi[j]^(-2))
    
    add.bivariate <-  paste(add.bivariate, paste(" + 
                     f(", paste("ii.", j, sep=""), ", model=\"iid2d\", n=2,
                     hyper = list(
                     prec1 = list(
                     initial =", init.prec.haz,", 
                     fixed = TRUE),
                     prec2 = list(
                     initial =", init.prec.waz,", 
                     fixed = TRUE),
                     cor = list(
                     initial = log((1+", corr, ")/(1-", corr, ")), 
                     fixed = TRUE)))"))
    
}

## Model formula ####
formula.bivariate.shared.bym <-  formula(paste("value ~ -1 + outcome + 
                                            f(admin1.haz, model = 'bym2',
                                              graph = admin1.mat, 
                                              scale.model = T, 
                                              constr = T,
                                              hyper = bym2_prior) +
                                            f(admin1.waz, model = 'bym2',
                                              graph = admin1.mat, 
                                              scale.model = T, 
                                              constr = T,
                                              hyper = bym2_prior) +
                                            f(admin1.haz.2, copy = \"admin1.waz\", 
                                              fixed = FALSE, hyper = lambda_prior)",
                                               paste(add.bivariate, collapse = " ")))

## linear combinations ####

# linear combination of preds without 2x2 REs
lc.vec.fe <- c(rep(1, n_regions))

diag.na.mat <- matrix(NA, nrow = n_regions, ncol = n_regions)
diag(diag.na.mat) <- 1
lc.vec.admin1.re <- diag.na.mat

## WAZ
lc.all.waz <- inla.make.lincombs(outcomeWAZ = lc.vec.fe, 
                                 admin1.waz = lc.vec.admin1.re)
names(lc.all.waz) <- gsub("lc", "waz.reg.", names(lc.all.waz))

## HAZ shared
lc.all.haz.shared <- inla.make.lincombs(outcomeHAZ = lc.vec.fe, 
                                        admin1.haz = lc.vec.admin1.re,
                                        admin1.haz.2 = lc.vec.admin1.re)
names(lc.all.haz.shared) <- gsub("lc", "haz.reg.", names(lc.all.haz.shared))


for(s in 1:n_sim) {
    
    cat(paste0("Starting sim ", s, "..."))
    # simulate direct estimates
    y_hat_r <- matrix(NA, nrow = n_regions, ncol = 2)
    V_hat_r <- array(NA, dim = dim(V_r))
    for (i in 1:n_regions) {
        y_hat_r[i,] <- rmvnorm(1, mu_r[i,], V_r[i, , ])
    
        # simulate V_des
        V_hat_r[i,,] <- rWishart(1, (n_r_star[i] - 1), V_r[i,,])/(n_r_star[i] - 1)
    }
    
    # calculate bias results for direct estimates
    sim_results$direct_estimates[1, s, ] <- as.vector(y_hat_r - mu_r)
    sim_results$direct_estimates[2, s, ] <- abs(as.vector(y_hat_r) - as.vector(mu_r))
    sim_results$direct_estimates[3, s, ] <- abs(as.vector(y_hat_r) - as.vector(mu_r))/abs(as.vector(mu_r))
    
    # calculate CIs
    ci_80_direct <- t(rbind(as.vector(y_hat_r), as.vector(y_hat_r)) + t(t(c(-1, 1))) %*% t(qnorm(0.9)*sqrt(c(V_hat_r[, 1, 1], V_hat_r[, 2, 2]))))
    ci_95_direct <- t(rbind(as.vector(y_hat_r), as.vector(y_hat_r)) + t(t(c(-1, 1))) %*% t(qnorm(0.975)*sqrt(c(V_hat_r[, 1, 1], V_hat_r[, 2, 2]))))
    
    # calculate coverage and width results
    sim_results$direct_estimates[4, s, ] <- (as.vector(mu_r) >= ci_80_direct[,1]) & (as.vector(mu_r) <= ci_80_direct[,2])
    sim_results$direct_estimates[5, s, ] <- (as.vector(mu_r) >= ci_95_direct[,1]) & (as.vector(mu_r) <= ci_95_direct[,2])
    sim_results$direct_estimates[6, s, ] <- (ci_80_direct[,2] - ci_80_direct[,1])
    sim_results$direct_estimates[7, s, ] <- (ci_95_direct[,2] - ci_95_direct[,1])
    
    # prep for inla model
    ## calculate correlation
    num_corrs <- dim(V_hat_r)[1]
    my.corrs_hat <- rep(NA, num_corrs)
    for (j in 1:num_corrs) {
        Vtmp <- V_hat_r[j,,]
        D <- diag(sqrt(diag(Vtmp)))
        DInv <- solve(D)
        corr.tmp <- DInv %*% Vtmp %*% DInv
        my.corrs_hat[j] <- corr.tmp[1,2]
    }
    results <- tibble(admin1 = 1:n_regions,
                      meanHAZ.bi = y_hat_r[, 1],
                      meanWAZ.bi = y_hat_r[, 2],
                      seHAZ.bi = sqrt(V_hat_r[,1,1]),
                      seWAZ.bi = sqrt(V_hat_r[,2,2]),
                      corr.bi = my.corrs_hat)
    results.long <- results %>% select(admin1,
                                       meanHAZ.bi, meanWAZ.bi) %>%
        pivot_longer(cols = c(meanHAZ.bi, meanWAZ.bi),
                     names_to = "outcome",
                     names_prefix = "mean",
                     values_to = "value")
    results.long$outcome <- ifelse(results.long$outcome == "HAZ.bi", "HAZ", "WAZ")
    results.long$admin1.haz <- ifelse(results.long$outcome == "HAZ", results.long$admin1, NA)
    results.long$admin1.waz <- ifelse(results.long$outcome == "WAZ", results.long$admin1, NA)
    results.long$obs <- 1:nrow(results.long)
    
    # create a list of the data
    data <- as.list(results.long)
    data$weights <- rep(1, n_regions)
    
    Nt <- length(unique(data$admin1))
    N <- length(data$admin1)
    n_regions <- Nt
    
    ## Define the index-vectors ii.1 ii.2 etc, which are the
    ## index's for the iid2d-model at timepoint 1, 2, ...
    for(j in 1:Nt) {
        itmp <-  numeric(N)
        itmp[] <-  NA
        itmp[(j-1)*2 + 1] <-  1
        itmp[(j-1)*2 + 2] <-  2
        data <-  c(list(itmp), data)
        names(data)[1] <-  paste("ii.", j, sep="")
    }
    
    ## add another index for the shared component
    data$admin1.haz.2 <- data$admin1.haz
    
    ## add indeces for iid components in besag + iid formulation
    data$admin1.haz.iid <- data$admin1.haz
    data$admin1.waz.iid <- data$admin1.waz
    
    # Fit models ####
    cat(paste(" fitting ivariate shared BYM model... "))
    mod.bivariate.shared.bym <- fitINLA(formula = formula.bivariate.shared.bym, 
                                        data = data, 
                                        lincombs = c(lc.all.haz.shared, lc.all.waz))

    fitted_res <- mod.bivariate.shared.bym$summary.lincomb.derived[grep("reg",rownames(mod.bivariate.shared.bym$summary.lincomb.derived)),]
    latent_means <- fitted_res$`0.5quant`

    # calculate bias results for direct estimates
    sim_results$bivariate_shared_bym[1, s, ] <- latent_means - as.vector(mu_r)
    sim_results$bivariate_shared_bym[2, s, ] <- abs(latent_means - as.vector(mu_r))
    sim_results$bivariate_shared_bym[3, s, ] <- abs(latent_means - as.vector(mu_r))/abs(as.vector(mu_r))

    # calculate coverage and width results
    sim_results$bivariate_shared_bym[4, s, ] <- (as.vector(mu_r) >= fitted_res$`0.1quant`) & (as.vector(mu_r) <= fitted_res$`0.9quant`)
    sim_results$bivariate_shared_bym[5, s, ] <- (as.vector(mu_r) >= fitted_res$`0.025quant`) & (as.vector(mu_r) <= fitted_res$`0.975quant`)
    sim_results$bivariate_shared_bym[6, s, ] <- (fitted_res$`0.9quant` - fitted_res$`0.1quant`)
    sim_results$bivariate_shared_bym[7, s, ] <- (fitted_res$`0.975quant` - fitted_res$`0.025quant`)
    
    cat(paste0("done! \n"))
}

# compile results
sim_results_region <- lapply(sim_results, function(x) {t(apply(x, c(1,3), mean))})
sim_results_compiled <- lapply(sim_results_region, function(x) {apply(x, 2, mean)})

# plot region-level results
par(mfrow = c(ceiling(length(measures)/2), 2))
for (i in 1:length(measures)) {
    plot(sim_results_region$bivariate_shared_bym[, i] ~ sim_results_region$direct_estimates[, i],
         pch = 19, col = rgb(0, 0, 0, alpha = 0.5), xlab = "direct estimates", ylab = "IID models",
         main = measures[i])
    abline(0, 1, col = "darkgreen")
}
```

<!-- ### Data generating mechanism -->

<!-- Let $r = 1, \dots, R$ index region. The region-level means will be generated as -->

<!-- \eq{ -->
<!--     \mu_{r} &= \beta_1 + v_{r} + u_{r} -->
<!-- } -->

<!-- with $\beta_1$ set to the values estimated from a model fit to the 2014 KDHS HAZ data. For the random effects $v_{r}$ and $u_{r}$, we will first generate $v_{r}^* \overset{iid}{\sim} N(0,1)$, an unstructured random effect with fixed standard deviation 1, and $\bm{u}^*$ simulated from an ICAR using algorithm 3.1 in Rue and Held (2005). We also want the marginal variance of these ICAR random effects to be on the same scale as the IID random effects, so we must calculate a multiplicative factor $q$ that scales the random effects such that the geometric mean of the marginal variances equals 1. This scaling factor is calculated using the inverse precision of the ICAR model which is calculated from the adjacency matrix, as in Riebler et al (2016). -->

<!-- Then, we calculate -->

<!-- \eq{ -->
<!--     v_{r} + u_{r} &= \sigma(\sqrt{1 - \rho}v_{r}^* + (\sqrt{\rho / q}) u_{r}^*) -->
<!-- } -->

<!-- with parameters $\sigma$, the total standard deviation of the random effects, and $\rho$, the percent of the random effects that are spatial set to be the values estimated from a model fit to the 2014 KDHS HAZ data. -->

<!-- These random effects will only be simulated once and the same ones will be used across all simulations. -->

<!-- Once we have these $\mu_{r}$ values, we will simulate the area-level sample means as -->

<!-- \eq{ -->
<!--     \hat{y}_r | \mu_r, V_r &\sim N(\mu_r, V_r) -->
<!-- } -->

<!-- where the $V_r$ are set to be equal to the asymptotic design-based variance estimate, $\hat{V}^{des}_r$, of the area-level mean HAZ from the 2014 KDHS data. -->

<!-- Now, for an SRS, the sampling distribution of the area-level sample variances is -->

<!-- \eq{ -->
<!--     \hat{V}^{srs}_r | V_r, n_r \sim \Gamma(\frac{n_r - 1}{2}, \frac{n_r - 1}{2 V_r})  -->
<!-- } -->

<!-- where $n_r$ is the sample size of an SRS in area $r$. The 2014 KDHS uses a stratified cluster design rather than an SRS, which has a different sampling variance than an SRS. The ratio of the variance for a statistic calculated using a specific survey design to the variance of that statistic calculated using an SRS of the same sample size is called the design effect, -->

<!-- \eq{ -->
<!--     d^2 &= \frac{\hat{V}}{\hat{V}^srs}. -->
<!-- } -->

<!-- In the a typical DHS, the average design effect across all indicators is $d^2 = 1.5^2 = 2.25$ (https://userforum.dhsprogram.com/index.php?t=msg&goto=3448&S=Google). Thus, in our simulation we will set $n_r$ to be the sample size of children with HAZ scores in each region and adjust the variance of the sample variance accordingly. Since the variance of a $\Gamma(\alpha, \beta)$ distribution is $\alpha/\beta^2$, we have $\Var(\hat{V}_r) = \frac{\left(\frac{n_r-1}{2}\right)}{\left(\frac{n_r-1}{2 V_r}\right)^2} = \frac{2 (V_r)^2}{(n_r - 1)}$. Now, we want $\hat{V}_r = 2.25 \hat{V}^{srs}_r = 2.25 \frac{2 (V_r)^2}{(n_r - 1)} = \frac{2 (V_r)^2}{\left((n_r - 1)/2.25\right)} \approx \frac{2 (V_r)^2}{\left((n_r/2.25 - 1)\right)}$, which yields an effective sample size of $n_r/2$. Thus, we will simulate the area-level sample variances as -->

<!-- \eq{ -->
<!--     \hat{V}_r | V_r, n_r \sim \Gamma(\frac{(n_r/2.25 - 1)}{2}, \frac{n_r/2.25 - 1}{2 V_r}). -->
<!-- } -->

<!-- In each simulation, we will have the same values of $\mu_r$ and $V_r$ and we will simulate the $\hat{y}_r$ and $\hat{V}_r$. We will treat the $\hat{y}_r$ as pseudo-direct estimates and calculate 95\% confidence intervals as $\hat{y}_r \pm t_{n_r/2.25, 0.975}\sqrt{\hat{V}_r}$. We will also fit a suite of second stage smoothing models to the pseudo-direct estimates to estimate the latent means in each region with corresponding 95\% credible intervals. These models are: -->

<!-- 1. IID only -->
<!-- 2. IID + ICAR (i.e. the true data generating mechanism) -->

<!-- # 2-dimensional data -->

<!-- We will now move on to the bivariate case. -->

<!-- # Data generating mechanism -->

<!-- Let $c = 1, 2$ index the outcomes. The region-level means will be generated from fixed and random effects as -->

<!-- \eq{ -->
<!--     \mu_{r1} &= \beta_1 + v_{r1} + u_{r1} + \lambda (v_{r2} + u_{r2}) \\ -->
<!--     \mu_{r2} &= \beta_2 + v_{r2} + u_{r2} -->
<!-- } -->

<!-- with $\beta_1$ and $\beta_2$ set to the values estimated from our shared component model fit to the 2014 KDHS HAZ/WAZ data. For the random effects $v_{rc}$ and $u_{rc}$, we will first generate $v_{rc}^* \overset{iid}{\sim} N(0,1)$, an unstructured random effect with fixed standard deviation 1, and $\bm{u}_{c}^*$ simulated from an ICAR using algorithm 3.1 in Rue and Held (2005) and scaled so $\mbox{Var}(u_{rc}^*) \approx 1$, which is done by scaling the model so the geometric mean of these variances is 1, using the adjacency matrix to calculate the inverse precision of the ICAR model as in Riebler et al (2016). -->

<!-- Then, we calculate -->

<!-- \eq{ -->
<!--     v_{rc} + u_{rc} = \sigma_c(\sqrt{1 - \rho_c}v_{rc}^* + \sqrt{\rho_c}u_{rc}^*) -->
<!-- } -->

<!-- with parameters $\sigma_c$ and $\rho_c$ set to be the values estimated from our shared component model fit to the 2014 KDHS HAZ/WAZ data. -->

<!-- These random effects will only be simulated once and the same ones will be used across all simulations -->

<!-- Once we have these $\mu_{rc}$ values, we will simulate the  -->

